{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.3 64-bit ('venv')",
   "metadata": {
    "interpreter": {
     "hash": "a0961b9797cbc896f2f22c0eeb7c6d5d29355ea410cebab174a9d079b2400e95"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('JuJingYi', 95, 22, 3), ('yuihatano', 90, 28, 1), ('angelababy', 90, 27, 2)]\n"
     ]
    }
   ],
   "source": [
    "from operator import gt\n",
    "from pyspark.sql import SparkSession\n",
    "class Girl:\n",
    "    def __init__(self, faceValue, age):\n",
    "        self.faceValue = faceValue\n",
    "        self.age = age\n",
    "\n",
    "    def __gt__(self, other):\n",
    "        if other.faceValue == self.faceValue:\n",
    "            return gt(self.age, other.age)\n",
    "        else:\n",
    "            return gt(self.faceValue, other.faceValue)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    spark = SparkSession.builder.appName(\"PythonWordCount\").master(\"local\").getOrCreate()\n",
    "    sc = spark.sparkContext\n",
    "    rdd1 = sc.parallelize([(\"yuihatano\", 90, 28, 1), (\"angelababy\", 90, 27, 2), (\"JuJingYi\", 95, 22, 3)])\n",
    "    rdd2 = rdd1.sortBy(lambda das: Girl(das[1], das[2]),False)\n",
    "    print(rdd2.collect())\n",
    "    sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "10\n1\n4\n9\n16\nhello\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "aggregate() missing 1 required positional argument: 'combOp'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-f393bb68bb40>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m sumCount=nums.aggregate((0,0),\n\u001b[1;32m     16\u001b[0m     (lambda acc,value:(acc[0]+value,acc[1]+1),\n\u001b[0;32m---> 17\u001b[0;31m     (lambda acc1,acc2:(acc1[0]+acc2[0],acc1[1]+acc2[1]))))\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msumCount\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msumCount\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: aggregate() missing 1 required positional argument: 'combOp'"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"ender\")\n",
    "sc = SparkContext(conf = conf)\n",
    "\n",
    "squared = nums.map(lambda x: x * x).collect()\n",
    "sum = nums.reduce(lambda x, y: x+y)\n",
    "print(sum)\n",
    "for num in squared:\n",
    "    print(\"%i\" % num)\n",
    "lines = sc.parallelize([\"hello world\",\"hi\"])\n",
    "words = lines.flatMap(lambda line: line.split(\" \"))\n",
    "print(words.first())\n",
    "\n",
    "\n",
    "\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "10\n",
      "4\n",
      "2.5\n",
      "[('学习', 2), ('pyStack', 1), ('Python', 1)]\n",
      "defaultdict(<class 'int'>, {'Python': 1, '学习': 2, 'pyStack': 1})\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"PythonWordCount\").master(\"local\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "lines = sc.textFile(\"README.MD\")\n",
    "nums = sc.parallelize([1,2,3,4])\n",
    "sumCount=nums.aggregate((0,0),\n",
    "    (lambda acc,value:(acc[0]+value,acc[1]+1)),\n",
    "    (lambda acc1,acc2:(acc1[0]+acc2[0],acc1[1]+acc2[1])))\n",
    "print(sumCount[0])\n",
    "print(sumCount[1])\n",
    "print(sumCount[0]/sumCount[1])\n",
    "words = lines.flatMap(lambda x: x.split(\" \"))\n",
    "result2 = words.map(lambda x:(x,1)).reduceByKey(lambda x,y:x+y)\n",
    "result3 = lines.flatMap(lambda x: x.split(\" \")).countByValue()\n",
    "print(result2.top(3))\n",
    "print(result3)\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkSession\n",
    "spark = SparkSession.builder.appName(\"PythonWordCount\").master(\"local\").getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  }
 ]
}